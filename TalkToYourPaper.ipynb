{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnWs9ySGhD0b7cSVZU2qO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunp17/TalkToData-LLM-LangChain-Streamlit/blob/main/TalkToYourPaper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Install required Python packages"
      ],
      "metadata": {
        "id": "6X5vAVwtVT-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrLqczeX6lgA"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install unstructured\n",
        "!pip install docarray\n",
        "!pip install tiktoken\n",
        "!pip install PyPDF2\n",
        "!pip install chromadb\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import Packages"
      ],
      "metadata": {
        "id": "7jvzM8DyVcKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import ArxivLoader, PyPDFLoader, UnstructuredFileLoader, OnlinePDFLoader, UnstructuredPDFLoader\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.agents import load_tools, initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain import SerpAPIWrapper"
      ],
      "metadata": {
        "id": "NMWv4nVsBjzV"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Setting-up APIs"
      ],
      "metadata": {
        "id": "80VJ2hBPVhVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = 'YOUR-OPENAI-API'\n",
        "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"SERPAPI_API_KEY\"] = 'YOUR-SERPAPI'"
      ],
      "metadata": {
        "id": "7isJzPaLB4_J"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load pdf file of the paper"
      ],
      "metadata": {
        "id": "9fkElLyqVost"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2112.10752.pdf\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "SrUUAABgCdco"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define LLM Model"
      ],
      "metadata": {
        "id": "H_utx8PaXmy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature = 0.0)"
      ],
      "metadata": {
        "id": "9KTVnOmAXrQ2"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construc vector database and QA Chain"
      ],
      "metadata": {
        "id": "od015iewm0YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "# Select embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "# Create a vectorstore from documents\n",
        "db = Chroma.from_documents(texts, embeddings)\n",
        "# Create retriever interface\n",
        "retriever = db.as_retriever()\n",
        "# Create QA chain\n",
        "qa_system = RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=retriever,verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB3EP6q_X1vv",
        "outputId": "a976eb2e-2b4e-42b8-8141-58e707f670c2"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the response for a query"
      ],
      "metadata": {
        "id": "lo2RFWZJm_nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=qa_system.run(\"Explain different training stages of the latent diffusion model\")\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "jq_9CjQXaDPF",
        "outputId": "adff9178-8a9c-4cc3-fdcc-1de6c75c8256"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The training of the latent diffusion model (LDM) involves two distinct stages: the perceptual compression stage and the semantic compression stage.\n\n1. Perceptual Compression Stage: In this stage, an autoencoder is trained to provide a lower-dimensional representation space that is perceptually equivalent to the data space. The objective is to remove high-frequency details from the input images while still preserving the overall perceptual quality. This stage focuses on compressing the data in a way that eliminates imperceptible details. The autoencoder is trained to reconstruct the input images from their compressed representations.\n\n2. Semantic Compression Stage: After the perceptual compression stage, the actual generative model is trained to learn the semantic and conceptual composition of the data. This stage aims to capture the higher-level features and variations in the data. The generative model is trained using diffusion models, which gradually denoise a normally distributed variable to learn the reverse process of a fixed Markov Chain. The generative model consists of a sequence of denoising autoencoders, which are trained to predict a denoised variant of their input. The objective is to learn the reverse process of the Markov Chain and approximate the true data distribution.\n\nThe training of the LDM involves training the autoencoder in the perceptual compression stage and then training the generative model in the semantic compression stage. The resulting LDM can be used for high-resolution image synthesis and other tasks such as inpainting and super-resolution. The LDM offers a computationally tractable approach to diffusion-based image synthesis and provides flexibility in modeling various image modalities."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " What happens when you ask about something outside the paper content? Eg., To compare this work with any other related work??"
      ],
      "metadata": {
        "id": "3jpbEtTqayj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=qa_system.run(\"Are there any new papers for Text-to-Audio generation based on latent diffusion models?, If true, give the details of those papers\")\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "XMpllttsb0_v",
        "outputId": "0cbebb7c-8554-4a4f-e3f5-5a77428f1294"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, there is no mention of any new papers specifically related to Text-to-Audio generation based on latent diffusion models. Therefore, it is not possible to provide any details about such papers."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " How to solve this? - Can be solved by simply adding a search functionality"
      ],
      "metadata": {
        "id": "pUdun-3TcFeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SerpAPIWrapper for search functionality\n",
        "search = SerpAPIWrapper()"
      ],
      "metadata": {
        "id": "SXiG04TRcRW1"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the QA Chain as a tool along with the Serp search tool"
      ],
      "metadata": {
        "id": "SDN1JKEyh-04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"Useful when you need to answer questions about current events or past events related to the scientic paper, which are not explicit contents of the paper.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"QA system\",\n",
        "        func=qa_system.run,\n",
        "        description=\"Useful when you need to answer questions from the contents of the given scientific paper.\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "vEOZXFU_dhXR"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct an Agent using the tools"
      ],
      "metadata": {
        "id": "Oj7HviedishY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=False)"
      ],
      "metadata": {
        "id": "zXCMsFufXsak"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=agent.run(\"Are there any new papers for Text-to-Audio generation based on latent diffusion models?, If true, give the details of those papers\")\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "3rFc5ZYfSyDf",
        "outputId": "2520815a-98ff-47c8-eb75-c791fb69ba77"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, there is a new paper called \"AudioLDM: Text-to-Audio Generation Based on Latent Diffusion Models\" which is a TTA system that is built on a latent space to learn the continuous audio representations from contrastive training."
          },
          "metadata": {}
        }
      ]
    }
  ]
}